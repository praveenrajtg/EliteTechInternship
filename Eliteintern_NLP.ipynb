{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z-NT0JTW50c"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries\n",
        "import os\n",
        "import nltk\n",
        "import nltk.corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install nltk library\n",
        "#NLTK includes various corpora (e.g., Brown Corpus, Gutenberg Corpus), lexical resources (e.g., WordNet), tokenizers, and pre-trained models.\n",
        "#These resources are not included in the default installation and need to be downloaded separately using nltk.download().\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGjTtCJ3XWlV",
        "outputId": "6647ac70-bed6-4572-d0ae-7f0c105fbe65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hTaij_vXfx9",
        "outputId": "51767ceb-e1ae-434e-9496-ff8d1f8f0326"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package english_wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/english_wordnet.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In NLP, \"corpora\" (plural of corpus) refers to large collections of text or speech data that are used for training and analyzing language models. NLTK provides various pre-installed corpora (e.g., Brown Corpus, Gutenberg Corpus) and tools to manage them.\n",
        "print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtsNYSx1XwSR",
        "outputId": "454a0ffa-3f3d-4e81-9607-e9b7d12c5a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['masc_tagged.zip', 'comparative_sentences.zip', 'pl196x.zip', 'genesis', 'english_wordnet.zip', 'framenet_v17.zip', 'unicode_samples.zip', 'subjectivity', 'ppattach', 'timit', 'problem_reports', 'floresta', 'nps_chat.zip', 'semcor.zip', 'pe08.zip', 'webtext.zip', 'ycoe', 'alpino', 'pil.zip', 'reuters.zip', 'brown_tei.zip', 'senseval', 'udhr', 'crubadan.zip', 'webtext', 'wordnet31.zip', 'sentence_polarity.zip', 'dolch', 'brown_tei', 'conll2002', 'omw.zip', 'machado.zip', 'names.zip', 'wordnet.zip', 'words.zip', 'qc', 'twitter_samples', 'europarl_raw', 'conll2002.zip', 'chat80', 'english_wordnet', 'product_reviews_2', 'pe08', 'brown', 'inaugural.zip', 'panlex_swadesh.zip', 'nonbreaking_prefixes', 'pil', 'sinica_treebank', 'bcp47.zip', 'wordnet2022.zip', 'udhr2', 'brown.zip', 'ptb', 'toolbox', 'product_reviews_2.zip', 'dolch.zip', 'biocreative_ppi', 'pros_cons', 'lin_thesaurus', 'senseval.zip', 'cess_cat', 'state_union', 'toolbox.zip', 'udhr.zip', 'ptb.zip', 'conll2007.zip', 'cess_esp', 'sentiwordnet.zip', 'europarl_raw.zip', 'lin_thesaurus.zip', 'twitter_samples.zip', 'nonbreaking_prefixes.zip', 'switchboard.zip', 'extended_omw.zip', 'chat80.zip', 'comparative_sentences', 'mac_morpho', 'sentiwordnet', 'cmudict', 'timit.zip', 'kimmo.zip', 'wordnet_ic', 'framenet_v17', 'cmudict.zip', 'verbnet', 'jeita.zip', 'inaugural', 'sinica_treebank.zip', 'mac_morpho.zip', 'rte.zip', 'dependency_treebank.zip', 'conll2000.zip', 'indian', 'udhr2.zip', 'smultron.zip', 'movie_reviews.zip', 'mte_teip5', 'treebank.zip', 'ieer.zip', 'opinion_lexicon.zip', 'biocreative_ppi.zip', 'cess_esp.zip', 'words', 'wordnet2022', 'city_database.zip', 'stopwords', 'shakespeare.zip', 'stopwords.zip', 'qc.zip', 'nombank.1.0.zip', 'gazetteers.zip', 'swadesh', 'ycoe.zip', 'abc.zip', 'verbnet3', 'gazetteers', 'city_database', 'unicode_samples', 'knbc.zip', 'gutenberg.zip', 'movie_reviews', 'wordnet2021.zip', 'opinion_lexicon', 'verbnet3.zip', 'comtrans.zip', 'kimmo', 'wordnet_ic.zip', 'indian.zip', 'abc', 'switchboard', 'pros_cons.zip', 'floresta.zip', 'genesis.zip', 'shakespeare', 'mte_teip5.zip', 'verbnet.zip', 'state_union.zip', 'propbank.zip', 'product_reviews_1.zip', 'universal_treebanks_v20.zip', 'sentence_polarity', 'dependency_treebank', 'paradigms.zip', 'rte', 'cess_cat.zip', 'alpino.zip', 'product_reviews_1', 'smultron', 'swadesh.zip', 'framenet_v15', 'treebank', 'omw-1.4.zip', 'crubadan', 'names', 'ieer', 'nps_chat', 'ppattach.zip', 'paradigms', 'problem_reports.zip', 'conll2000', 'framenet_v15.zip', 'pl196x', 'subjectivity.zip', 'gutenberg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To download and use the stopwords corpus:\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOdlaZX8YLd6",
        "outputId": "5bf92308-698c-4814-a736-242f189a01e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTT64zoeYNb4",
        "outputId": "dc8c21a3-1b5a-4f45-8011-a6936c94bb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the first 10 words\n",
        "print(brown.words()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHGKvJK9YSXM",
        "outputId": "602d5867-d924-4c08-d035-a52d61175ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total number of words in the Brown Corpus\n",
        "word_count = len(brown.words())\n",
        "print(f\"Total number of words in the Brown Corpus: {word_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH9_J208YUcM",
        "outputId": "29ad804b-6459-4109-e0c4-555e91fe664f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in the Brown Corpus: 1161192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To view all available categories\n",
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJbDTyulYWU5",
        "outputId": "6a9ef82f-900c-425e-f7d7-33abb9b06b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To count the words in a specific category (e.g., \"news\"):\n",
        "word_count_news = len(brown.words(categories='news'))\n",
        "print(f\"Total words in the 'news' category: {word_count_news}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TQt9QafYZEk",
        "outputId": "0ace122e-fb7b-462c-b831-f40f1f5d8c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in the 'news' category: 100554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To find the number of unique words (vocabulary size):\n",
        "unique_words = len(set(brown.words()))\n",
        "print(f\"Number of unique words in the Brown Corpus: {unique_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWNx9OL2YbfO",
        "outputId": "8aa39e00-c974-459b-adb0-640a985a3e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in the Brown Corpus: 56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AI = \" who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s\""
      ],
      "metadata": {
        "id": "niJ2bEKvYd61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(AI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wFBknI-YkHJ",
        "outputId": "80762175-ba1b-4247-f6d9-7a128b7f8df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the words\n",
        "from nltk.tokenize import word_tokenize\n",
        "AI_tokens = word_tokenize(AI)\n",
        "AI_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaZ2SYMoYnJN",
        "outputId": "03f07488-15f3-437a-aced-b5d272e3f815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who',\n",
              " 'hath',\n",
              " 'relieu',\n",
              " \"'\",\n",
              " 'd',\n",
              " 'you',\n",
              " '?',\n",
              " 'Fra',\n",
              " '.',\n",
              " 'Barnardo',\n",
              " 'ha',\n",
              " \"'\",\n",
              " 's',\n",
              " 'my',\n",
              " 'place',\n",
              " ':',\n",
              " 'giue',\n",
              " 'you',\n",
              " 'goodnight',\n",
              " '.',\n",
              " 'Exit',\n",
              " 'Fran',\n",
              " '.',\n",
              " 'Mar',\n",
              " '.',\n",
              " 'Holla',\n",
              " 'Barnardo',\n",
              " 'Bar',\n",
              " '.',\n",
              " 'Say',\n",
              " ',',\n",
              " 'what',\n",
              " 'is',\n",
              " 'Horatio',\n",
              " 'there',\n",
              " '?',\n",
              " 'Hor',\n",
              " '.',\n",
              " 'A',\n",
              " 'peece',\n",
              " 'of',\n",
              " 'him',\n",
              " 'Bar',\n",
              " '.',\n",
              " 'Welcome',\n",
              " 'Horatio',\n",
              " ',',\n",
              " 'welcome',\n",
              " 'good',\n",
              " 'Marcellus',\n",
              " 'Mar',\n",
              " '.',\n",
              " 'What',\n",
              " ',',\n",
              " 'ha',\n",
              " \"'\",\n",
              " 's',\n",
              " 'this',\n",
              " 'thing',\n",
              " 'appear',\n",
              " \"'\",\n",
              " 'd',\n",
              " 'againe',\n",
              " 'to',\n",
              " 'night',\n",
              " 'Bar',\n",
              " '.',\n",
              " 'I',\n",
              " 'haue',\n",
              " 'seene',\n",
              " 'nothing',\n",
              " 'Mar',\n",
              " '.',\n",
              " 'Horatio',\n",
              " 'saies',\n",
              " ',',\n",
              " \"'\",\n",
              " 'tis',\n",
              " 'but',\n",
              " 'our',\n",
              " 'Fantasie',\n",
              " ',',\n",
              " 'And',\n",
              " 'will',\n",
              " 'not',\n",
              " 'let',\n",
              " 'beleefe',\n",
              " 'take',\n",
              " 'hold',\n",
              " 'of',\n",
              " 'him',\n",
              " 'Touching',\n",
              " 'this',\n",
              " 'dreaded',\n",
              " 'sight',\n",
              " ',',\n",
              " 'twice',\n",
              " 'seene',\n",
              " 'of',\n",
              " 'vs',\n",
              " ',',\n",
              " 'Therefore',\n",
              " 'I',\n",
              " 'haue',\n",
              " 'intreated',\n",
              " 'him',\n",
              " 'along',\n",
              " 'With',\n",
              " 'vs',\n",
              " ',',\n",
              " 'to',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'minutes',\n",
              " 'of',\n",
              " 'this',\n",
              " 'Night',\n",
              " ',',\n",
              " 'That',\n",
              " 'if',\n",
              " 'againe',\n",
              " 'this',\n",
              " 'Apparition',\n",
              " 'come',\n",
              " ',',\n",
              " 'He',\n",
              " 'may',\n",
              " 'approue',\n",
              " 'our',\n",
              " 'eyes',\n",
              " ',',\n",
              " 'and',\n",
              " 'speake',\n",
              " 'to',\n",
              " 'it',\n",
              " 'Hor',\n",
              " '.',\n",
              " 'Tush',\n",
              " ',',\n",
              " 'tush',\n",
              " ',',\n",
              " \"'\",\n",
              " 'twill',\n",
              " 'not',\n",
              " 'appeare',\n",
              " 'Bar',\n",
              " '.',\n",
              " 'Sit',\n",
              " 'downe',\n",
              " 'a',\n",
              " '-',\n",
              " 'while',\n",
              " ',',\n",
              " 'And',\n",
              " 'let',\n",
              " 'vs',\n",
              " 'once',\n",
              " 'againe',\n",
              " 'assaile',\n",
              " 'your',\n",
              " 'eares',\n",
              " ',',\n",
              " 'That',\n",
              " 'are',\n",
              " 'so',\n",
              " 'fortified',\n",
              " 'against',\n",
              " 'our',\n",
              " 'Story',\n",
              " ',',\n",
              " 'What',\n",
              " 'we',\n",
              " 'two',\n",
              " 'Nights',\n",
              " 'haue',\n",
              " 'seene',\n",
              " 'Hor',\n",
              " '.',\n",
              " 'Well',\n",
              " ',',\n",
              " 'sit',\n",
              " 'we',\n",
              " 'downe',\n",
              " ',',\n",
              " 'And',\n",
              " 'let',\n",
              " 'vs',\n",
              " 'heare',\n",
              " 'Barnardo',\n",
              " 'speake',\n",
              " 'of',\n",
              " 'this',\n",
              " 'Barn',\n",
              " '.',\n",
              " 'Last',\n",
              " 'night',\n",
              " 'of',\n",
              " 'all',\n",
              " ',',\n",
              " 'When',\n",
              " 'yond',\n",
              " 'same',\n",
              " 'Starre',\n",
              " 'that',\n",
              " \"'\",\n",
              " 's']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()   # To count the frequency of words in a given text."
      ],
      "metadata": {
        "id": "XijR3IBBYui9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in AI_tokens:\n",
        "  fdist[word.lower()]+=1\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcN6lOjpY3fu",
        "outputId": "4791972d-ba42-436e-ddc8-e031898c90d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 19, '.': 14, \"'\": 7, 'of': 6, 'this': 5, 'bar': 4, 'and': 4, 'vs': 4, 'barnardo': 3, 's': 3, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "# used to generate word sequences (n-grams) from text."
      ],
      "metadata": {
        "id": "TR8uTIzFY5FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"The best and most beautiful things in the world cannot be seen or even touched, they must be felt with the heart\"\n",
        "quotes_tokens = nltk.word_tokenize(string)\n",
        "quotes_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYC4GEQWY_WQ",
        "outputId": "ba18a924-a949-413b-a479-06febfc84f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'best',\n",
              " 'and',\n",
              " 'most',\n",
              " 'beautiful',\n",
              " 'things',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'can',\n",
              " 'not',\n",
              " 'be',\n",
              " 'seen',\n",
              " 'or',\n",
              " 'even',\n",
              " 'touched',\n",
              " ',',\n",
              " 'they',\n",
              " 'must',\n",
              " 'be',\n",
              " 'felt',\n",
              " 'with',\n",
              " 'the',\n",
              " 'heart']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
        "quotes_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQVFGG_vZB6R",
        "outputId": "ec63e67e-9b8b-4bf1-b75c-f348eeb66d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best'),\n",
              " ('best', 'and'),\n",
              " ('and', 'most'),\n",
              " ('most', 'beautiful'),\n",
              " ('beautiful', 'things'),\n",
              " ('things', 'in'),\n",
              " ('in', 'the'),\n",
              " ('the', 'world'),\n",
              " ('world', 'can'),\n",
              " ('can', 'not'),\n",
              " ('not', 'be'),\n",
              " ('be', 'seen'),\n",
              " ('seen', 'or'),\n",
              " ('or', 'even'),\n",
              " ('even', 'touched'),\n",
              " ('touched', ','),\n",
              " (',', 'they'),\n",
              " ('they', 'must'),\n",
              " ('must', 'be'),\n",
              " ('be', 'felt'),\n",
              " ('felt', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to find stem word in the given words using stemming method\n",
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n",
        "print(pst.stem(\"having\"))\n",
        "words_to_stem=[\"give\", \"giving\", \"given\", \"gave\"]\n",
        "for words in words_to_stem:\n",
        "  print(words+ \":\" +pst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha5DoJ2OZECx",
        "outputId": "37a95d69-b303-4501-8c44-e720694ae899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n",
            "give:give\n",
            "giving:give\n",
            "given:given\n",
            "gave:gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization\n",
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_len=WordNetLemmatizer()\n",
        "word_len.lemmatize('corpora')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uH5S8u-YZMEG",
        "outputId": "8b1b3d1e-4366-4903-c74f-e8d52a20dbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in words_to_stem:\n",
        "  print(words+ \":\" +word_len.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL44aX-0ZdkM",
        "outputId": "6311fb1b-74de-409b-e992-39a06945b317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give:give\n",
            "giving:giving\n",
            "given:given\n",
            "gave:gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords in english\n",
        "from nltk.corpus import stopwords\n",
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sV0n0brZjFd",
        "outputId": "095c4f3e-7bf2-4416-8730-9a2bfa41eeea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yMCjVehZonz",
        "outputId": "e81eaf11-4cdc-4ad4-a6d8-74423d328076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "import re\n",
        "punctuation=re.compile(r'[-.?!,:;()|0-9]')"
      ],
      "metadata": {
        "id": "cmNORpHWZtKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenize\n",
        "sent = \"Timothy is a natural when it comes to drawing\"\n",
        "sent_tokens = word_tokenize(sent)"
      ],
      "metadata": {
        "id": "0wCCMhEaZx4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sQlixJCZ1og",
        "outputId": "27663bea-1eca-4890-bffb-8e32df148c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Timothy', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('natural', 'JJ')]\n",
            "[('when', 'WRB')]\n",
            "[('it', 'PRP')]\n",
            "[('comes', 'VBZ')]\n",
            "[('to', 'TO')]\n",
            "[('drawing', 'VBG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent2=\"John is eating a delicious cake\"\n",
        "sent2_tokens = word_tokenize(sent2)\n",
        "for token in sent2_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtkEku6CZ985",
        "outputId": "365ca414-ad0c-4bae-b1a7-c19f820f8b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'NNP')]\n",
            "[('is', 'VBZ')]\n",
            "[('eating', 'VBG')]\n",
            "[('a', 'DT')]\n",
            "[('delicious', 'JJ')]\n",
            "[('cake', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Example text\n",
        "text = \"I absolutely love this product! It's amazing.\"\n",
        "\n",
        "# Perform sentiment analysis\n",
        "scores = analyzer.polarity_scores(text)\n",
        "print(\"Sentiment Scores:\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5aS6w2aAT-",
        "outputId": "3ece82c2-415d-4360-f612-77d614ad82a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.368, 'pos': 0.632, 'compound': 0.862}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Get prompt (text) from the user\n",
        "user_input = input(\"Enter a sentence or text to analyze sentiment: \")\n",
        "\n",
        "# Perform sentiment analysis\n",
        "result = sentiment_analyzer(user_input)\n",
        "\n",
        "# Print the result\n",
        "print(\"Sentiment Analysis Result:\")\n",
        "for r in result:\n",
        "    print(f\"Label: {r['label']}, Confidence: {r['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tsf8JekaDfO",
        "outputId": "7b5d2857-e920-448c-eee7-2ea59e77a207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence or text to analyze sentiment: I love elite tech intership\n",
            "Sentiment Analysis Result:\n",
            "Label: POSITIVE, Confidence: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nzfMulUaQ49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}